## CNN结构

### 写在前面

**结构：**卷积层 —> 池化层 —> 卷积层 —> 池化层 —> 全连接层 —> 输出层

​        卷积层(Convolution):提取图像的底层特征，即将图像中**符合卷积核特征的特征**提取出来**（卷积核特征是网络自己学习出来的）**

​        池化层/降采样层/下采样层(Pooling/Subsampling):降低feature map的维度，防止过拟合。

​        全连接层(Fully connected)：将池化层的结果拉平(flatten)成一个长向量，汇总之前卷积层和池化层得到的底层的信息和特征。

​        输出层(Output):全连接+激活(二分类用sigmoid，多分类用SoftMax归一化)	

#### 卷积层：

- **卷积核（kernel）/ 特征提取器（filter）：**一般都是原图里包含的部分特征。**不需人工设置，通过反向传播、梯度下降自己找到**（若图像是多通道的，卷积核也是多通道）

- - 扩张/膨胀/空洞卷积（Dilation）：卷积核不再是连续的，而是跨像素的感受野，可以捕捉图像中更有效的信息
  - 转置卷积/反卷积/上采样：生成对抗网络（小图 —> 大图）

- **感受野（receptive field）：**卷积核在原图上光顾到的区域

- **padding：**防止边缘信息被丢失或忽略。为了识别边缘特征，通常在外圈补0（即zero padding）

- - same padding：保证输入、输出特征矩阵维度一样
  - full padding：保证边缘像素点和中间像素点被处理的次数是一样的

- **特征图（feature map）：**input和kernel对应相乘得到。**有多少个kernel就有多少个feature map**

- **偏置项：**往往是一个标量，直接加在原始的feature map上即可

**例：**对于一张具有三通道的RGB颜色的图像（大小为6×6×3）
具有三个颜色通道的卷积核（大小为3×3×3）
= 生成一个4×4大小的特征图  *(6-3+2\*0)/1 + 1 = 4* 

计算公式： $output =  (input-kernelsize+2*padding)/stride +1$(向下取整)

**Normalization：**如**ReLus**（Rectified Linear Units，修正线性单元激活函数）把feature map上负数的部分全部变为0

> 激活函数放在卷积后

#### 池化层

缩小得到的feature map，在保留feature map原先特征的同时，缩小数据量

- 最大池化（max pooling）：常用
- 平均池化（avg pooling）

**池化的作用：**

1. 减少参数量且保留原始特征（大图变为小图，但保留了大图的特征）
2. 防止过拟合
3. 可以为CNN带来**“平移不变性”**（强化学习训练一个打桩游戏，不能加入池化层，会丢失位置信息）

#### 全连接层

每个神经元都要乘以一定的权重，最后加和起来。隐藏层里可以有不止一个全连接层

> **CNN & 全连接神经网络 异同：**
> CNN和全连接神经网络都是通用函数的拟合器（黑箱），都为 input layer + hidden layer + output layer；
> 对比全连接神经网络，CNN是局部连接，且CNN的 hidden layer 增加了convolution和subsampling

#### 参数

**可训练参数个数：**

- 6个5×5卷积核：（5×5+1）×6 = 156 （+1为偏置项）
- 假设最后一层卷积得到的结果是5×5×16，flatten后为5×5×16=400。输入下一层全连接层（有120个神经元），可训练参数个数为：120×（400+1）=48120 （+1为偏置项）

**初始参数/超参：**

- 卷积核尺寸、数目
- 池化步长、大小
- 全连接层神经元的数量